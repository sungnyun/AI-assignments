{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "\n",
    "class UnFlatten(nn.Module):\n",
    "    def forward(self, input, size=100):\n",
    "        return input.view(input.size(0), size, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim1, h_dim2, z_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # encoder part\n",
    "        self.fc1 = nn.Linear(x_dim, h_dim1)\n",
    "        self.fc2 = nn.Linear(h_dim1, h_dim2)\n",
    "        self.fc31 = nn.Linear(h_dim2, z_dim)\n",
    "        self.fc32 = nn.Linear(h_dim2, z_dim)\n",
    "        # decoder part\n",
    "        self.fc4 = nn.Linear(z_dim, h_dim2)\n",
    "        self.fc5 = nn.Linear(h_dim2, h_dim1)\n",
    "        self.fc6 = nn.Linear(h_dim1, x_dim)\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return self.fc31(h), self.fc32(h) # mu, log_var\n",
    "    \n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu) # return z sample\n",
    "        \n",
    "    def decoder(self, z):\n",
    "        h = F.relu(self.fc4(z))\n",
    "        h = F.relu(self.fc5(h))\n",
    "        return F.sigmoid(self.fc6(h)) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encoder(x.view(-1, 784))\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decoder(z), mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(x_dim=784, h_dim1= 512, h_dim2=256, z_dim=2)\n",
    "if torch.cuda.is_available():\n",
    "    vae.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(vae.parameters())\n",
    "def loss_function(recon_x, x, mu, log_var):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + 10 * KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        recon_batch, mu, log_var = vae(data)\n",
    "        loss = loss_function(recon_batch, data, mu, log_var)\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item() / len(data)))\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    vae.eval()\n",
    "    test_loss= 0\n",
    "    with torch.no_grad():\n",
    "        for data, _ in test_loader:\n",
    "            data = data.cuda()\n",
    "            recon, mu, log_var = vae(data)\n",
    "            \n",
    "            # sum up batch loss\n",
    "            test_loss += loss_function(recon, data, mu, log_var).item()\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40078ef5189e46be93f5b22272b0ef25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 544.767761\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 204.787262\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 195.727570\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 194.143356\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 192.049622\n",
      "====> Epoch: 1 Average loss: 205.1307\n",
      "====> Test set loss: 192.7945\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 194.481812\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 200.449219\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 189.427338\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 193.465744\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 195.833740\n",
      "====> Epoch: 2 Average loss: 191.4079\n",
      "====> Test set loss: 190.6421\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 188.033035\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 191.334106\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 188.374237\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 191.181808\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 183.888016\n",
      "====> Epoch: 3 Average loss: 189.5318\n",
      "====> Test set loss: 188.7500\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 190.267136\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 188.494995\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 185.856171\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 184.129028\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 190.849335\n",
      "====> Epoch: 4 Average loss: 188.2595\n",
      "====> Test set loss: 187.7227\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 187.372437\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 187.791733\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 193.320908\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 194.254608\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 197.295578\n",
      "====> Epoch: 5 Average loss: 187.5780\n",
      "====> Test set loss: 187.1112\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 201.114456\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 182.776550\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 181.691727\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 188.832458\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 191.891357\n",
      "====> Epoch: 6 Average loss: 187.0155\n",
      "====> Test set loss: 186.1965\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 186.701752\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 182.949371\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 180.366150\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 191.607269\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 188.106018\n",
      "====> Epoch: 7 Average loss: 186.6446\n",
      "====> Test set loss: 185.8432\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 192.242874\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 184.007095\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 192.448227\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 178.345764\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 183.728210\n",
      "====> Epoch: 8 Average loss: 186.1861\n",
      "====> Test set loss: 185.8456\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 191.144867\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 179.000610\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 181.534943\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 178.557129\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 192.892090\n",
      "====> Epoch: 9 Average loss: 185.6800\n",
      "====> Test set loss: 185.3806\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 174.385651\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 188.641571\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 192.021240\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 183.343292\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 186.268234\n",
      "====> Epoch: 10 Average loss: 185.2661\n",
      "====> Test set loss: 185.1349\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 191.551163\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 182.474319\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 181.681152\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 182.006958\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 189.310257\n",
      "====> Epoch: 11 Average loss: 185.0022\n",
      "====> Test set loss: 184.6427\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 188.281479\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 191.010742\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 179.188766\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 183.807159\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 181.896439\n",
      "====> Epoch: 12 Average loss: 184.5710\n",
      "====> Test set loss: 184.1479\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 182.609680\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 181.924637\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 182.840759\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 182.445801\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 181.988129\n",
      "====> Epoch: 13 Average loss: 184.3184\n",
      "====> Test set loss: 184.1066\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 184.561340\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 178.480316\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 185.360245\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 188.688889\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 181.129837\n",
      "====> Epoch: 14 Average loss: 184.0733\n",
      "====> Test set loss: 183.8774\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 180.936584\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 189.059265\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 181.517349\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 179.788910\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 193.326126\n",
      "====> Epoch: 15 Average loss: 183.7597\n",
      "====> Test set loss: 183.5196\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 185.780472\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 190.814133\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 181.761230\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 182.869751\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 184.368744\n",
      "====> Epoch: 16 Average loss: 183.6211\n",
      "====> Test set loss: 184.1886\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 186.371246\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 186.591553\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 181.121368\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 191.382645\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 181.391266\n",
      "====> Epoch: 17 Average loss: 183.4510\n",
      "====> Test set loss: 183.3044\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 189.813324\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 178.647781\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 181.055832\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 184.496246\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 176.455627\n",
      "====> Epoch: 18 Average loss: 183.3068\n",
      "====> Test set loss: 183.6589\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 178.994095\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 180.841614\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 189.442368\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 180.222397\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 184.887589\n",
      "====> Epoch: 19 Average loss: 183.1550\n",
      "====> Test set loss: 183.0385\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 175.778336\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 181.835114\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 174.140656\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 190.057373\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 178.131744\n",
      "====> Epoch: 20 Average loss: 183.1464\n",
      "====> Test set loss: 182.9983\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 184.307587\n",
      "Train Epoch: 21 [12800/60000 (21%)]\tLoss: 179.175400\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 192.504852\n",
      "Train Epoch: 21 [38400/60000 (64%)]\tLoss: 178.940933\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 182.676636\n",
      "====> Epoch: 21 Average loss: 182.9224\n",
      "====> Test set loss: 182.5985\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 186.511185\n",
      "Train Epoch: 22 [12800/60000 (21%)]\tLoss: 186.283813\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 188.415924\n",
      "Train Epoch: 22 [38400/60000 (64%)]\tLoss: 185.171326\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 183.296463\n",
      "====> Epoch: 22 Average loss: 182.9510\n",
      "====> Test set loss: 182.8699\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 181.157654\n",
      "Train Epoch: 23 [12800/60000 (21%)]\tLoss: 183.321320\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 184.117813\n",
      "Train Epoch: 23 [38400/60000 (64%)]\tLoss: 188.403458\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 184.532852\n",
      "====> Epoch: 23 Average loss: 182.7600\n",
      "====> Test set loss: 182.6633\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 184.186340\n",
      "Train Epoch: 24 [12800/60000 (21%)]\tLoss: 187.353027\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 180.633316\n",
      "Train Epoch: 24 [38400/60000 (64%)]\tLoss: 180.675446\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 182.120529\n",
      "====> Epoch: 24 Average loss: 182.6878\n",
      "====> Test set loss: 182.7958\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 185.350418\n",
      "Train Epoch: 25 [12800/60000 (21%)]\tLoss: 174.000214\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 180.552521\n",
      "Train Epoch: 25 [38400/60000 (64%)]\tLoss: 177.023407\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 188.306335\n",
      "====> Epoch: 25 Average loss: 182.5823\n",
      "====> Test set loss: 182.0612\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 186.272217\n",
      "Train Epoch: 26 [12800/60000 (21%)]\tLoss: 189.886200\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 180.270706\n",
      "Train Epoch: 26 [38400/60000 (64%)]\tLoss: 181.859192\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 189.324692\n",
      "====> Epoch: 26 Average loss: 182.3635\n",
      "====> Test set loss: 182.7295\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 182.921143\n",
      "Train Epoch: 27 [12800/60000 (21%)]\tLoss: 178.661301\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 184.753265\n",
      "Train Epoch: 27 [38400/60000 (64%)]\tLoss: 187.099869\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 182.490005\n",
      "====> Epoch: 27 Average loss: 182.4237\n",
      "====> Test set loss: 182.4570\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 184.972488\n",
      "Train Epoch: 28 [12800/60000 (21%)]\tLoss: 182.263092\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 182.810333\n",
      "Train Epoch: 28 [38400/60000 (64%)]\tLoss: 183.829346\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 171.870636\n",
      "====> Epoch: 28 Average loss: 182.3346\n",
      "====> Test set loss: 182.3446\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 186.043091\n",
      "Train Epoch: 29 [12800/60000 (21%)]\tLoss: 188.841858\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 181.710037\n",
      "Train Epoch: 29 [38400/60000 (64%)]\tLoss: 176.310745\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 180.974030\n",
      "====> Epoch: 29 Average loss: 182.2527\n",
      "====> Test set loss: 181.8803\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 185.609894\n",
      "Train Epoch: 30 [12800/60000 (21%)]\tLoss: 180.613953\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 189.373703\n",
      "Train Epoch: 30 [38400/60000 (64%)]\tLoss: 180.305176\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 188.515915\n",
      "====> Epoch: 30 Average loss: 182.2783\n",
      "====> Test set loss: 182.1573\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 187.211700\n",
      "Train Epoch: 31 [12800/60000 (21%)]\tLoss: 182.484375\n",
      "Train Epoch: 31 [25600/60000 (43%)]\tLoss: 184.187576\n",
      "Train Epoch: 31 [38400/60000 (64%)]\tLoss: 178.067245\n",
      "Train Epoch: 31 [51200/60000 (85%)]\tLoss: 176.361160\n",
      "====> Epoch: 31 Average loss: 182.1540\n",
      "====> Test set loss: 182.4083\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 181.428757\n",
      "Train Epoch: 32 [12800/60000 (21%)]\tLoss: 179.422440\n",
      "Train Epoch: 32 [25600/60000 (43%)]\tLoss: 181.961212\n",
      "Train Epoch: 32 [38400/60000 (64%)]\tLoss: 181.887421\n",
      "Train Epoch: 32 [51200/60000 (85%)]\tLoss: 176.920639\n",
      "====> Epoch: 32 Average loss: 182.1179\n",
      "====> Test set loss: 181.9881\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 183.912643\n",
      "Train Epoch: 33 [12800/60000 (21%)]\tLoss: 187.621002\n",
      "Train Epoch: 33 [25600/60000 (43%)]\tLoss: 178.299728\n",
      "Train Epoch: 33 [38400/60000 (64%)]\tLoss: 181.763885\n",
      "Train Epoch: 33 [51200/60000 (85%)]\tLoss: 175.648056\n",
      "====> Epoch: 33 Average loss: 181.9454\n",
      "====> Test set loss: 182.1105\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 185.939423\n",
      "Train Epoch: 34 [12800/60000 (21%)]\tLoss: 183.873413\n",
      "Train Epoch: 34 [25600/60000 (43%)]\tLoss: 180.707748\n",
      "Train Epoch: 34 [38400/60000 (64%)]\tLoss: 185.029526\n",
      "Train Epoch: 34 [51200/60000 (85%)]\tLoss: 188.202621\n",
      "====> Epoch: 34 Average loss: 182.0476\n",
      "====> Test set loss: 182.0765\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 185.598984\n",
      "Train Epoch: 35 [12800/60000 (21%)]\tLoss: 178.067505\n",
      "Train Epoch: 35 [25600/60000 (43%)]\tLoss: 183.454407\n",
      "Train Epoch: 35 [38400/60000 (64%)]\tLoss: 181.327133\n",
      "Train Epoch: 35 [51200/60000 (85%)]\tLoss: 182.452515\n",
      "====> Epoch: 35 Average loss: 181.8601\n",
      "====> Test set loss: 181.6804\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 178.375641\n",
      "Train Epoch: 36 [12800/60000 (21%)]\tLoss: 169.723984\n",
      "Train Epoch: 36 [25600/60000 (43%)]\tLoss: 173.346893\n",
      "Train Epoch: 36 [38400/60000 (64%)]\tLoss: 183.878738\n",
      "Train Epoch: 36 [51200/60000 (85%)]\tLoss: 180.742126\n",
      "====> Epoch: 36 Average loss: 181.8763\n",
      "====> Test set loss: 181.7157\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 179.272980\n",
      "Train Epoch: 37 [12800/60000 (21%)]\tLoss: 186.815552\n",
      "Train Epoch: 37 [25600/60000 (43%)]\tLoss: 178.441284\n",
      "Train Epoch: 37 [38400/60000 (64%)]\tLoss: 186.323303\n",
      "Train Epoch: 37 [51200/60000 (85%)]\tLoss: 180.721542\n",
      "====> Epoch: 37 Average loss: 181.8134\n",
      "====> Test set loss: 181.9870\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 178.360153\n",
      "Train Epoch: 38 [12800/60000 (21%)]\tLoss: 183.037323\n",
      "Train Epoch: 38 [25600/60000 (43%)]\tLoss: 183.762939\n",
      "Train Epoch: 38 [38400/60000 (64%)]\tLoss: 178.652557\n",
      "Train Epoch: 38 [51200/60000 (85%)]\tLoss: 181.241180\n",
      "====> Epoch: 38 Average loss: 181.8173\n",
      "====> Test set loss: 182.1490\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 182.551956\n",
      "Train Epoch: 39 [12800/60000 (21%)]\tLoss: 184.233597\n",
      "Train Epoch: 39 [25600/60000 (43%)]\tLoss: 177.178406\n",
      "Train Epoch: 39 [38400/60000 (64%)]\tLoss: 189.986053\n",
      "Train Epoch: 39 [51200/60000 (85%)]\tLoss: 174.537582\n",
      "====> Epoch: 39 Average loss: 181.9093\n",
      "====> Test set loss: 181.6679\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 176.656235\n",
      "Train Epoch: 40 [12800/60000 (21%)]\tLoss: 184.259247\n",
      "Train Epoch: 40 [25600/60000 (43%)]\tLoss: 193.839844\n",
      "Train Epoch: 40 [38400/60000 (64%)]\tLoss: 186.055786\n",
      "Train Epoch: 40 [51200/60000 (85%)]\tLoss: 177.805603\n",
      "====> Epoch: 40 Average loss: 181.7662\n",
      "====> Test set loss: 181.8085\n",
      "Train Epoch: 41 [0/60000 (0%)]\tLoss: 179.776611\n",
      "Train Epoch: 41 [12800/60000 (21%)]\tLoss: 189.902222\n",
      "Train Epoch: 41 [25600/60000 (43%)]\tLoss: 177.464600\n",
      "Train Epoch: 41 [38400/60000 (64%)]\tLoss: 179.624161\n",
      "Train Epoch: 41 [51200/60000 (85%)]\tLoss: 180.786163\n",
      "====> Epoch: 41 Average loss: 181.7341\n",
      "====> Test set loss: 181.7800\n",
      "Train Epoch: 42 [0/60000 (0%)]\tLoss: 182.686172\n",
      "Train Epoch: 42 [12800/60000 (21%)]\tLoss: 189.580490\n",
      "Train Epoch: 42 [25600/60000 (43%)]\tLoss: 184.072479\n",
      "Train Epoch: 42 [38400/60000 (64%)]\tLoss: 184.850677\n",
      "Train Epoch: 42 [51200/60000 (85%)]\tLoss: 177.441025\n",
      "====> Epoch: 42 Average loss: 181.5562\n",
      "====> Test set loss: 181.8334\n",
      "Train Epoch: 43 [0/60000 (0%)]\tLoss: 185.404144\n",
      "Train Epoch: 43 [12800/60000 (21%)]\tLoss: 179.638443\n",
      "Train Epoch: 43 [25600/60000 (43%)]\tLoss: 187.900543\n",
      "Train Epoch: 43 [38400/60000 (64%)]\tLoss: 184.994019\n",
      "Train Epoch: 43 [51200/60000 (85%)]\tLoss: 184.874420\n",
      "====> Epoch: 43 Average loss: 181.6394\n",
      "====> Test set loss: 182.1547\n",
      "Train Epoch: 44 [0/60000 (0%)]\tLoss: 186.656021\n",
      "Train Epoch: 44 [12800/60000 (21%)]\tLoss: 181.910110\n",
      "Train Epoch: 44 [25600/60000 (43%)]\tLoss: 184.060242\n",
      "Train Epoch: 44 [38400/60000 (64%)]\tLoss: 170.415649\n",
      "Train Epoch: 44 [51200/60000 (85%)]\tLoss: 181.679199\n",
      "====> Epoch: 44 Average loss: 181.5089\n",
      "====> Test set loss: 181.9158\n",
      "Train Epoch: 45 [0/60000 (0%)]\tLoss: 180.935471\n",
      "Train Epoch: 45 [12800/60000 (21%)]\tLoss: 182.247330\n",
      "Train Epoch: 45 [25600/60000 (43%)]\tLoss: 179.315460\n",
      "Train Epoch: 45 [38400/60000 (64%)]\tLoss: 180.303192\n",
      "Train Epoch: 45 [51200/60000 (85%)]\tLoss: 180.131104\n",
      "====> Epoch: 45 Average loss: 181.6339\n",
      "====> Test set loss: 181.7908\n",
      "Train Epoch: 46 [0/60000 (0%)]\tLoss: 186.314789\n",
      "Train Epoch: 46 [12800/60000 (21%)]\tLoss: 184.302368\n",
      "Train Epoch: 46 [25600/60000 (43%)]\tLoss: 186.986633\n",
      "Train Epoch: 46 [38400/60000 (64%)]\tLoss: 183.658905\n",
      "Train Epoch: 46 [51200/60000 (85%)]\tLoss: 178.764389\n",
      "====> Epoch: 46 Average loss: 181.4162\n",
      "====> Test set loss: 181.2576\n",
      "Train Epoch: 47 [0/60000 (0%)]\tLoss: 182.765259\n",
      "Train Epoch: 47 [12800/60000 (21%)]\tLoss: 177.040192\n",
      "Train Epoch: 47 [25600/60000 (43%)]\tLoss: 177.752716\n",
      "Train Epoch: 47 [38400/60000 (64%)]\tLoss: 179.819397\n",
      "Train Epoch: 47 [51200/60000 (85%)]\tLoss: 185.588287\n",
      "====> Epoch: 47 Average loss: 181.5360\n",
      "====> Test set loss: 181.9355\n",
      "Train Epoch: 48 [0/60000 (0%)]\tLoss: 179.904785\n",
      "Train Epoch: 48 [12800/60000 (21%)]\tLoss: 181.100006\n",
      "Train Epoch: 48 [25600/60000 (43%)]\tLoss: 175.307419\n",
      "Train Epoch: 48 [38400/60000 (64%)]\tLoss: 177.510834\n",
      "Train Epoch: 48 [51200/60000 (85%)]\tLoss: 183.808350\n",
      "====> Epoch: 48 Average loss: 181.4467\n",
      "====> Test set loss: 181.3529\n",
      "Train Epoch: 49 [0/60000 (0%)]\tLoss: 185.735672\n",
      "Train Epoch: 49 [12800/60000 (21%)]\tLoss: 180.697968\n",
      "Train Epoch: 49 [25600/60000 (43%)]\tLoss: 185.705078\n",
      "Train Epoch: 49 [38400/60000 (64%)]\tLoss: 178.846802\n",
      "Train Epoch: 49 [51200/60000 (85%)]\tLoss: 186.457367\n",
      "====> Epoch: 49 Average loss: 181.5245\n",
      "====> Test set loss: 181.4681\n",
      "Train Epoch: 50 [0/60000 (0%)]\tLoss: 180.289917\n",
      "Train Epoch: 50 [12800/60000 (21%)]\tLoss: 178.166992\n",
      "Train Epoch: 50 [25600/60000 (43%)]\tLoss: 177.928207\n",
      "Train Epoch: 50 [38400/60000 (64%)]\tLoss: 181.374100\n",
      "Train Epoch: 50 [51200/60000 (85%)]\tLoss: 192.608719\n",
      "====> Epoch: 50 Average loss: 181.3990\n",
      "====> Test set loss: 181.8262\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "for epoch in tqdm(range(1, 51)):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([441, 2])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "#     z = torch.randn(64, 2).cuda()\n",
    "    x = np.linspace(-1, 1, 21)\n",
    "    z = []\n",
    "    for i in x:\n",
    "        for j in x:\n",
    "            z.append([j, -i])\n",
    "            \n",
    "    z = torch.tensor(z).cuda()\n",
    "    \n",
    "#     print (z.shape)\n",
    "    sample = vae.decoder(z).cuda()\n",
    "    \n",
    "    save_image(sample.view(21*21, 1, 28, 28), './samples/sample_1' + '.png', nrow=21)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
