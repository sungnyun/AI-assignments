{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "\n",
    "class UnFlatten(nn.Module):\n",
    "    def forward(self, input, size=100):\n",
    "        return input.view(input.size(0), size, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim1, h_dim2, z_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # encoder part\n",
    "        self.fc1 = nn.Linear(x_dim, h_dim1)\n",
    "        self.fc2 = nn.Linear(h_dim1, h_dim2)\n",
    "        self.fc31 = nn.Linear(h_dim2, z_dim)\n",
    "        self.fc32 = nn.Linear(h_dim2, z_dim)\n",
    "        # decoder part\n",
    "        self.fc4 = nn.Linear(z_dim, h_dim2)\n",
    "        self.fc5 = nn.Linear(h_dim2, h_dim1)\n",
    "        self.fc6 = nn.Linear(h_dim1, x_dim)\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return self.fc31(h), self.fc32(h) # mu, log_var\n",
    "    \n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu) # return z sample\n",
    "        \n",
    "    def decoder(self, z):\n",
    "        h = F.relu(self.fc4(z))\n",
    "        h = F.relu(self.fc5(h))\n",
    "        return F.sigmoid(self.fc6(h)) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encoder(x.view(-1, 784))\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decoder(z), mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7388685bd93f4b358cb9458e86449de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data/MNIST/raw/train-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de88a79e10f045e9a510fb294bbb2cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7228753f7ff43a29c82e732a1010463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1901bd07839b4f5bb10d9792a442f75a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(x_dim=784, h_dim1= 512, h_dim2=256, z_dim=2)\n",
    "if torch.cuda.is_available():\n",
    "    vae.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(vae.parameters())\n",
    "def loss_function(recon_x, x, mu, log_var):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + 10 * KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        recon_batch, mu, log_var = vae(data)\n",
    "        loss = loss_function(recon_batch, data, mu, log_var)\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item() / len(data)))\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    vae.eval()\n",
    "    test_loss= 0\n",
    "    with torch.no_grad():\n",
    "        for data, _ in test_loader:\n",
    "            data = data.cuda()\n",
    "            recon, mu, log_var = vae(data)\n",
    "            \n",
    "            # sum up batch loss\n",
    "            test_loss += loss_function(recon, data, mu, log_var).item()\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a1f77d634534b3b8940031601fc57e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sungnyun/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 545.758362\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 206.957504\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 199.324249\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 193.465942\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 190.826050\n",
      "====> Epoch: 1 Average loss: 204.7116\n",
      "====> Test set loss: 192.9540\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 199.535004\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 192.805542\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 183.428757\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 191.394409\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 192.336716\n",
      "====> Epoch: 2 Average loss: 191.2883\n",
      "====> Test set loss: 190.4596\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 185.559723\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 192.515289\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 189.219406\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 186.504990\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 194.436432\n",
      "====> Epoch: 3 Average loss: 189.5730\n",
      "====> Test set loss: 188.5932\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 189.824722\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 191.201889\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 187.138199\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 182.815155\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 188.117462\n",
      "====> Epoch: 4 Average loss: 188.3174\n",
      "====> Test set loss: 187.8969\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 189.147873\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 191.301544\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 189.965942\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 189.626663\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 190.367676\n",
      "====> Epoch: 5 Average loss: 187.6359\n",
      "====> Test set loss: 187.0478\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 192.651749\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 186.151215\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 189.824509\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 183.896667\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 187.184387\n",
      "====> Epoch: 6 Average loss: 187.0396\n",
      "====> Test set loss: 186.4170\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 185.524750\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 191.896759\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 194.389160\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 191.784943\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 184.282059\n",
      "====> Epoch: 7 Average loss: 186.5789\n",
      "====> Test set loss: 185.9345\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 188.494324\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 185.653625\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 188.797806\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 187.804626\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 186.347412\n",
      "====> Epoch: 8 Average loss: 186.0777\n",
      "====> Test set loss: 185.9836\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 184.450806\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 185.106552\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 186.709229\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 184.485367\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 190.393036\n",
      "====> Epoch: 9 Average loss: 185.8395\n",
      "====> Test set loss: 185.1394\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 195.832153\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 182.826782\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 177.632721\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 186.134033\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 194.962372\n",
      "====> Epoch: 10 Average loss: 185.5205\n",
      "====> Test set loss: 185.5444\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 191.059860\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 183.933105\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 186.771240\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 185.459885\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 180.588593\n",
      "====> Epoch: 11 Average loss: 185.2115\n",
      "====> Test set loss: 184.9282\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 177.709137\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 181.565247\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 182.465912\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 181.224457\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 184.438705\n",
      "====> Epoch: 12 Average loss: 184.9849\n",
      "====> Test set loss: 184.4295\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 190.404465\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 187.742508\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 182.238800\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 191.986206\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 192.026947\n",
      "====> Epoch: 13 Average loss: 184.6862\n",
      "====> Test set loss: 184.2375\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 187.026001\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 188.345444\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 193.978333\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 188.820770\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 179.954788\n",
      "====> Epoch: 14 Average loss: 184.4926\n",
      "====> Test set loss: 184.1520\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 186.595901\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 182.242920\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 179.285156\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 183.364380\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 185.877274\n",
      "====> Epoch: 15 Average loss: 184.2686\n",
      "====> Test set loss: 184.3694\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 179.465179\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 183.457657\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 185.373901\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 181.303406\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 178.348083\n",
      "====> Epoch: 16 Average loss: 184.0732\n",
      "====> Test set loss: 183.9858\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 186.910065\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 178.950150\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 180.007019\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 182.444763\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 178.864014\n",
      "====> Epoch: 17 Average loss: 183.9385\n",
      "====> Test set loss: 184.0761\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 184.770416\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 185.265167\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 182.994720\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 175.560898\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 186.731277\n",
      "====> Epoch: 18 Average loss: 183.7859\n",
      "====> Test set loss: 183.4739\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 182.259003\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 176.646973\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 185.070145\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 182.215897\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 186.001099\n",
      "====> Epoch: 19 Average loss: 183.6711\n",
      "====> Test set loss: 183.6257\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 181.488388\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 183.535767\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 183.180771\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 183.352875\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 187.487259\n",
      "====> Epoch: 20 Average loss: 183.4814\n",
      "====> Test set loss: 184.1122\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 180.515991\n",
      "Train Epoch: 21 [12800/60000 (21%)]\tLoss: 178.748917\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 184.222580\n",
      "Train Epoch: 21 [38400/60000 (64%)]\tLoss: 180.928284\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 182.151520\n",
      "====> Epoch: 21 Average loss: 183.4445\n",
      "====> Test set loss: 183.1468\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 178.579437\n",
      "Train Epoch: 22 [12800/60000 (21%)]\tLoss: 187.384048\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 193.684372\n",
      "Train Epoch: 22 [38400/60000 (64%)]\tLoss: 188.129257\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 181.828644\n",
      "====> Epoch: 22 Average loss: 183.1836\n",
      "====> Test set loss: 183.1499\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 184.106964\n",
      "Train Epoch: 23 [12800/60000 (21%)]\tLoss: 175.779755\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 183.097870\n",
      "Train Epoch: 23 [38400/60000 (64%)]\tLoss: 181.447357\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 191.859009\n",
      "====> Epoch: 23 Average loss: 183.1569\n",
      "====> Test set loss: 182.8870\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 185.308823\n",
      "Train Epoch: 24 [12800/60000 (21%)]\tLoss: 181.546600\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 184.695435\n",
      "Train Epoch: 24 [38400/60000 (64%)]\tLoss: 180.937469\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 183.818222\n",
      "====> Epoch: 24 Average loss: 183.2890\n",
      "====> Test set loss: 183.3868\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 179.946106\n",
      "Train Epoch: 25 [12800/60000 (21%)]\tLoss: 180.737473\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 175.335938\n",
      "Train Epoch: 25 [38400/60000 (64%)]\tLoss: 186.171738\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 190.990540\n",
      "====> Epoch: 25 Average loss: 182.9667\n",
      "====> Test set loss: 183.1024\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 186.930832\n",
      "Train Epoch: 26 [12800/60000 (21%)]\tLoss: 183.320724\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 191.028717\n",
      "Train Epoch: 26 [38400/60000 (64%)]\tLoss: 173.712357\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 186.752045\n",
      "====> Epoch: 26 Average loss: 182.8384\n",
      "====> Test set loss: 182.8207\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 188.427963\n",
      "Train Epoch: 27 [12800/60000 (21%)]\tLoss: 182.986465\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 187.182083\n",
      "Train Epoch: 27 [38400/60000 (64%)]\tLoss: 180.474716\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 182.787430\n",
      "====> Epoch: 27 Average loss: 182.7484\n",
      "====> Test set loss: 182.9698\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 180.922882\n",
      "Train Epoch: 28 [12800/60000 (21%)]\tLoss: 187.230377\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 177.791473\n",
      "Train Epoch: 28 [38400/60000 (64%)]\tLoss: 186.125046\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 178.166702\n",
      "====> Epoch: 28 Average loss: 182.7939\n",
      "====> Test set loss: 182.6881\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 182.115005\n",
      "Train Epoch: 29 [12800/60000 (21%)]\tLoss: 188.171890\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 176.074158\n",
      "Train Epoch: 29 [38400/60000 (64%)]\tLoss: 182.445923\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 179.656708\n",
      "====> Epoch: 29 Average loss: 182.7452\n",
      "====> Test set loss: 182.6948\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 178.181854\n",
      "Train Epoch: 30 [12800/60000 (21%)]\tLoss: 173.977585\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 185.744202\n",
      "Train Epoch: 30 [38400/60000 (64%)]\tLoss: 179.642578\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 188.599762\n",
      "====> Epoch: 30 Average loss: 182.7207\n",
      "====> Test set loss: 182.7190\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 184.781372\n",
      "Train Epoch: 31 [12800/60000 (21%)]\tLoss: 188.521820\n",
      "Train Epoch: 31 [25600/60000 (43%)]\tLoss: 182.094208\n",
      "Train Epoch: 31 [38400/60000 (64%)]\tLoss: 174.969757\n",
      "Train Epoch: 31 [51200/60000 (85%)]\tLoss: 190.455200\n",
      "====> Epoch: 31 Average loss: 182.4889\n",
      "====> Test set loss: 182.6874\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 180.857376\n",
      "Train Epoch: 32 [12800/60000 (21%)]\tLoss: 181.039001\n",
      "Train Epoch: 32 [25600/60000 (43%)]\tLoss: 183.836777\n",
      "Train Epoch: 32 [38400/60000 (64%)]\tLoss: 177.939850\n",
      "Train Epoch: 32 [51200/60000 (85%)]\tLoss: 177.454239\n",
      "====> Epoch: 32 Average loss: 182.5007\n",
      "====> Test set loss: 182.1955\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 172.939529\n",
      "Train Epoch: 33 [12800/60000 (21%)]\tLoss: 180.127594\n",
      "Train Epoch: 33 [25600/60000 (43%)]\tLoss: 187.444305\n",
      "Train Epoch: 33 [38400/60000 (64%)]\tLoss: 180.717529\n",
      "Train Epoch: 33 [51200/60000 (85%)]\tLoss: 179.952484\n",
      "====> Epoch: 33 Average loss: 182.4973\n",
      "====> Test set loss: 182.9854\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 180.486130\n",
      "Train Epoch: 34 [12800/60000 (21%)]\tLoss: 189.806396\n",
      "Train Epoch: 34 [25600/60000 (43%)]\tLoss: 181.500549\n",
      "Train Epoch: 34 [38400/60000 (64%)]\tLoss: 185.478012\n",
      "Train Epoch: 34 [51200/60000 (85%)]\tLoss: 188.047394\n",
      "====> Epoch: 34 Average loss: 182.4592\n",
      "====> Test set loss: 182.4272\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 181.865707\n",
      "Train Epoch: 35 [12800/60000 (21%)]\tLoss: 185.484177\n",
      "Train Epoch: 35 [25600/60000 (43%)]\tLoss: 190.840073\n",
      "Train Epoch: 35 [38400/60000 (64%)]\tLoss: 184.750458\n",
      "Train Epoch: 35 [51200/60000 (85%)]\tLoss: 183.589737\n",
      "====> Epoch: 35 Average loss: 182.5022\n",
      "====> Test set loss: 182.6505\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 180.674530\n",
      "Train Epoch: 36 [12800/60000 (21%)]\tLoss: 184.349930\n",
      "Train Epoch: 36 [25600/60000 (43%)]\tLoss: 184.996445\n",
      "Train Epoch: 36 [38400/60000 (64%)]\tLoss: 180.692825\n",
      "Train Epoch: 36 [51200/60000 (85%)]\tLoss: 180.762741\n",
      "====> Epoch: 36 Average loss: 182.4452\n",
      "====> Test set loss: 182.5125\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 180.002182\n",
      "Train Epoch: 37 [12800/60000 (21%)]\tLoss: 179.507507\n",
      "Train Epoch: 37 [25600/60000 (43%)]\tLoss: 182.941711\n",
      "Train Epoch: 37 [38400/60000 (64%)]\tLoss: 179.348679\n",
      "Train Epoch: 37 [51200/60000 (85%)]\tLoss: 181.788528\n",
      "====> Epoch: 37 Average loss: 182.2619\n",
      "====> Test set loss: 182.2152\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 182.985657\n",
      "Train Epoch: 38 [12800/60000 (21%)]\tLoss: 176.193130\n",
      "Train Epoch: 38 [25600/60000 (43%)]\tLoss: 180.198044\n",
      "Train Epoch: 38 [38400/60000 (64%)]\tLoss: 183.813934\n",
      "Train Epoch: 38 [51200/60000 (85%)]\tLoss: 185.805435\n",
      "====> Epoch: 38 Average loss: 182.2039\n",
      "====> Test set loss: 182.1650\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 179.404984\n",
      "Train Epoch: 39 [12800/60000 (21%)]\tLoss: 178.264877\n",
      "Train Epoch: 39 [25600/60000 (43%)]\tLoss: 187.326843\n",
      "Train Epoch: 39 [38400/60000 (64%)]\tLoss: 185.393158\n",
      "Train Epoch: 39 [51200/60000 (85%)]\tLoss: 183.409210\n",
      "====> Epoch: 39 Average loss: 182.0671\n",
      "====> Test set loss: 182.1188\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 185.171677\n",
      "Train Epoch: 40 [12800/60000 (21%)]\tLoss: 175.877487\n",
      "Train Epoch: 40 [25600/60000 (43%)]\tLoss: 183.452179\n",
      "Train Epoch: 40 [38400/60000 (64%)]\tLoss: 173.855560\n",
      "Train Epoch: 40 [51200/60000 (85%)]\tLoss: 183.159836\n",
      "====> Epoch: 40 Average loss: 182.0028\n",
      "====> Test set loss: 181.9390\n",
      "Train Epoch: 41 [0/60000 (0%)]\tLoss: 176.959244\n",
      "Train Epoch: 41 [12800/60000 (21%)]\tLoss: 191.847351\n",
      "Train Epoch: 41 [25600/60000 (43%)]\tLoss: 183.722733\n",
      "Train Epoch: 41 [38400/60000 (64%)]\tLoss: 180.074448\n",
      "Train Epoch: 41 [51200/60000 (85%)]\tLoss: 183.553360\n",
      "====> Epoch: 41 Average loss: 182.0669\n",
      "====> Test set loss: 182.0214\n",
      "Train Epoch: 42 [0/60000 (0%)]\tLoss: 174.831390\n",
      "Train Epoch: 42 [12800/60000 (21%)]\tLoss: 176.050735\n",
      "Train Epoch: 42 [25600/60000 (43%)]\tLoss: 185.828156\n",
      "Train Epoch: 42 [38400/60000 (64%)]\tLoss: 181.310638\n",
      "Train Epoch: 42 [51200/60000 (85%)]\tLoss: 188.308746\n",
      "====> Epoch: 42 Average loss: 182.0691\n",
      "====> Test set loss: 182.1281\n",
      "Train Epoch: 43 [0/60000 (0%)]\tLoss: 178.518539\n",
      "Train Epoch: 43 [12800/60000 (21%)]\tLoss: 179.747406\n",
      "Train Epoch: 43 [25600/60000 (43%)]\tLoss: 177.499878\n",
      "Train Epoch: 43 [38400/60000 (64%)]\tLoss: 174.018005\n",
      "Train Epoch: 43 [51200/60000 (85%)]\tLoss: 185.720016\n",
      "====> Epoch: 43 Average loss: 181.9924\n",
      "====> Test set loss: 182.3444\n",
      "Train Epoch: 44 [0/60000 (0%)]\tLoss: 180.598755\n",
      "Train Epoch: 44 [12800/60000 (21%)]\tLoss: 176.495911\n",
      "Train Epoch: 44 [25600/60000 (43%)]\tLoss: 184.141953\n",
      "Train Epoch: 44 [38400/60000 (64%)]\tLoss: 184.460800\n",
      "Train Epoch: 44 [51200/60000 (85%)]\tLoss: 184.675751\n",
      "====> Epoch: 44 Average loss: 182.0080\n",
      "====> Test set loss: 182.0191\n",
      "Train Epoch: 45 [0/60000 (0%)]\tLoss: 179.196808\n",
      "Train Epoch: 45 [12800/60000 (21%)]\tLoss: 177.915207\n",
      "Train Epoch: 45 [25600/60000 (43%)]\tLoss: 185.485596\n",
      "Train Epoch: 45 [38400/60000 (64%)]\tLoss: 183.243988\n",
      "Train Epoch: 45 [51200/60000 (85%)]\tLoss: 182.602463\n",
      "====> Epoch: 45 Average loss: 181.9558\n",
      "====> Test set loss: 182.5604\n",
      "Train Epoch: 46 [0/60000 (0%)]\tLoss: 178.264923\n",
      "Train Epoch: 46 [12800/60000 (21%)]\tLoss: 176.413940\n",
      "Train Epoch: 46 [25600/60000 (43%)]\tLoss: 184.355453\n",
      "Train Epoch: 46 [38400/60000 (64%)]\tLoss: 178.014404\n",
      "Train Epoch: 46 [51200/60000 (85%)]\tLoss: 181.339783\n",
      "====> Epoch: 46 Average loss: 182.1079\n",
      "====> Test set loss: 182.5489\n",
      "Train Epoch: 47 [0/60000 (0%)]\tLoss: 184.036179\n",
      "Train Epoch: 47 [12800/60000 (21%)]\tLoss: 183.019089\n",
      "Train Epoch: 47 [25600/60000 (43%)]\tLoss: 188.872787\n",
      "Train Epoch: 47 [38400/60000 (64%)]\tLoss: 184.102615\n",
      "Train Epoch: 47 [51200/60000 (85%)]\tLoss: 183.650116\n",
      "====> Epoch: 47 Average loss: 182.1604\n",
      "====> Test set loss: 182.1262\n",
      "Train Epoch: 48 [0/60000 (0%)]\tLoss: 184.257141\n",
      "Train Epoch: 48 [12800/60000 (21%)]\tLoss: 174.790924\n",
      "Train Epoch: 48 [25600/60000 (43%)]\tLoss: 181.025269\n",
      "Train Epoch: 48 [38400/60000 (64%)]\tLoss: 184.314148\n",
      "Train Epoch: 48 [51200/60000 (85%)]\tLoss: 184.442535\n",
      "====> Epoch: 48 Average loss: 181.8637\n",
      "====> Test set loss: 181.9284\n",
      "Train Epoch: 49 [0/60000 (0%)]\tLoss: 188.087601\n",
      "Train Epoch: 49 [12800/60000 (21%)]\tLoss: 178.572723\n",
      "Train Epoch: 49 [25600/60000 (43%)]\tLoss: 174.451660\n",
      "Train Epoch: 49 [38400/60000 (64%)]\tLoss: 176.042236\n",
      "Train Epoch: 49 [51200/60000 (85%)]\tLoss: 190.212433\n",
      "====> Epoch: 49 Average loss: 181.8928\n",
      "====> Test set loss: 181.8078\n",
      "Train Epoch: 50 [0/60000 (0%)]\tLoss: 175.133606\n",
      "Train Epoch: 50 [12800/60000 (21%)]\tLoss: 183.049225\n",
      "Train Epoch: 50 [25600/60000 (43%)]\tLoss: 182.597549\n",
      "Train Epoch: 50 [38400/60000 (64%)]\tLoss: 181.932526\n",
      "Train Epoch: 50 [51200/60000 (85%)]\tLoss: 173.973663\n",
      "====> Epoch: 50 Average loss: 182.0318\n",
      "====> Test set loss: 181.9572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "for epoch in tqdm(range(1, 51)):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "#     z = torch.randn(64, 2).cuda()\n",
    "    x = np.linspace(-1, 1, 21)\n",
    "    z = []\n",
    "    for i in x:\n",
    "        for j in x:\n",
    "            z.append([j, -i])\n",
    "            \n",
    "    z = torch.tensor(z).cuda()\n",
    "    \n",
    "#     print (z.shape)\n",
    "    sample = vae.decoder(z.float()).cuda()\n",
    "    \n",
    "    save_image(sample.view(21*21, 1, 28, 28), './samples/sample_1' + '.png', nrow=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
