{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(nin, nout, stride=1, padding=1):\n",
    "    return nn.Conv2d(nin, nout, kernel_size=3, stride=stride, padding=padding, bias=False)\n",
    "\n",
    "\n",
    "class residual_Layer(nn.Module):\n",
    "    def __init__(self, nin, nout, stride=1, padding=1, subsample=False):\n",
    "        super(residual_Layer, self).__init__()\n",
    "        self.subsample = subsample\n",
    "        self.nin = nin\n",
    "        self.nout = nout\n",
    "        if self.subsample is False:\n",
    "            self.conv1 = conv3x3(self.nin, self.nout)\n",
    "        else:\n",
    "            self.conv1 = conv3x3(self.nin, self.nout, stride=2)\n",
    "            self.subsample_layer = conv3x3(self.nin, self.nout, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(self.nout)\n",
    "        self.bn2 = nn.BatchNorm2d(self.nout)\n",
    "        self.conv2 = conv3x3(self.nout, self.nout)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.subsample is not False:\n",
    "            residual = self.subsample_layer(residual)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, nlayer):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.nin = 16\n",
    "        self.nlayer = nlayer\n",
    "        self.block1 = self.block(16, subsample=False)\n",
    "        self.block2 = self.block(32, subsample=True)\n",
    "        self.block3 = self.block(64, subsample=True)\n",
    "        self.conv1 = conv3x3(3, 16)\n",
    "        self.bn = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.avg = nn.AvgPool2d(8)\n",
    "        self.fc = nn.Linear(64, 10)\n",
    "        \n",
    "    def block(self, nout, subsample):\n",
    "        layers = []\n",
    "        layers.append(residual_Layer(self.nin, nout, subsample=subsample))\n",
    "        for i in range(self.nlayer - 1):\n",
    "            layers.append(residual_Layer(nout, nout))\n",
    "        self.nin = nout\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.avg(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    \n",
    "def ResNet20():\n",
    "    return ResNet(nlayer=3)\n",
    "\n",
    "def ResNet32():\n",
    "    return ResNet(nlayer=5)\n",
    "\n",
    "def ResNet44():\n",
    "    return ResNet(nlayer=7)\n",
    "\n",
    "def ResNet56():\n",
    "    return ResNet(nlayer=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the seoncd experiment, made plain network without residual\n",
    "class plain_Layer(nn.Module):\n",
    "    def __init__(self, nin, nout, stride=1, padding=1, subsample=False):\n",
    "        super(plain_Layer, self).__init__()\n",
    "        self.subsample = subsample\n",
    "        self.nin = nin\n",
    "        self.nout = nout\n",
    "        if self.subsample is False:\n",
    "            self.conv1 = conv3x3(self.nin, self.nout)\n",
    "        else:\n",
    "            self.conv1 = conv3x3(self.nin, self.nout, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(self.nout)\n",
    "        self.bn2 = nn.BatchNorm2d(self.nout)\n",
    "        self.conv2 = conv3x3(self.nout, self.nout)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    \n",
    "class PlainNet(nn.Module):\n",
    "    def __init__(self, nlayer):\n",
    "        super(PlainNet, self).__init__()\n",
    "        self.nlayer = nlayer\n",
    "        self.nin = 16\n",
    "        self.block1 = self.block(16, subsample=False)\n",
    "        self.block2 = self.block(32, subsample=True)\n",
    "        self.block3 = self.block(64, subsample=True)\n",
    "        self.conv1 = conv3x3(3, 16)\n",
    "        self.bn = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.avg = nn.AvgPool2d(8)\n",
    "        self.fc = nn.Linear(64, 10)\n",
    "        \n",
    "    def block(self, nout, subsample):\n",
    "        layers = []\n",
    "        layers.append(plain_Layer(self.nin, nout, subsample=subsample))\n",
    "        for i in range(self.nlayer - 1):\n",
    "            layers.append(plain_Layer(nout, nout))\n",
    "        self.nin = nout\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.avg(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    \n",
    "def Plain20():\n",
    "    return PlainNet(nlayer=3)\n",
    "\n",
    "def Plain32():\n",
    "    return PlainNet(nlayer=5)\n",
    "\n",
    "def Plain44():\n",
    "    return PlainNet(nlayer=7)\n",
    "\n",
    "def Plain56():\n",
    "    return PlainNet(nlayer=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
